<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Donghyun Kim">

<title>Domain Adaptation for 3D Cell Segmentation – 3dCellSegDA</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">3dCellSegDA</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://donghyunkm.github.io/"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#background-on-3d-cell-segmentation" id="toc-background-on-3d-cell-segmentation" class="nav-link active" data-scroll-target="#background-on-3d-cell-segmentation">Background on 3D Cell Segmentation</a></li>
  <li><a href="#problem-overview" id="toc-problem-overview" class="nav-link" data-scroll-target="#problem-overview">Problem Overview</a></li>
  <li><a href="#embedseg" id="toc-embedseg" class="nav-link" data-scroll-target="#embedseg">EmbedSeg</a></li>
  <li><a href="#datasets" id="toc-datasets" class="nav-link" data-scroll-target="#datasets">Datasets</a></li>
  <li><a href="#transfer-learning" id="toc-transfer-learning" class="nav-link" data-scroll-target="#transfer-learning">Transfer Learning</a></li>
  <li><a href="#adversarial-domain-adaptation" id="toc-adversarial-domain-adaptation" class="nav-link" data-scroll-target="#adversarial-domain-adaptation">Adversarial Domain Adaptation</a></li>
  <li><a href="#style-transfer" id="toc-style-transfer" class="nav-link" data-scroll-target="#style-transfer">Style Transfer</a></li>
  <li><a href="#conclusion-and-future-work" id="toc-conclusion-and-future-work" class="nav-link" data-scroll-target="#conclusion-and-future-work">Conclusion and Future Work</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Domain Adaptation for 3D Cell Segmentation</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Donghyun Kim </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<p>Instance segmentation of 3D cell microscopy data is crucial for studying embryonic development. Various deep learning models have been developed for this task, such as Cellpose <span class="citation" data-cites="stringer2021cellpose">(<a href="#ref-stringer2021cellpose" role="doc-biblioref">Stringer et al. 2021</a>)</span> and EmbedSeg <span class="citation" data-cites="lalit2022embedseg">(<a href="#ref-lalit2022embedseg" role="doc-biblioref">Lalit, Tomancak, and Jug 2022</a>)</span>.</p>
<p>However, these models are only as good as the data they are trained on, and ground-truth annotations are difficult to obtain. One solution is to train models on existing ground truths from other datasets and experiments. However, differences in experimental protocols cause images obtained in separate environments to exhibit varying data distributions. A model trained on these differing source distributions may encounter difficulties generalizing to target distributions.</p>
<p>We explore various domain adaptation methods to solve this issue.</p>
<section id="background-on-3d-cell-segmentation" class="level3">
<h3 class="anchored" data-anchor-id="background-on-3d-cell-segmentation">Background on 3D Cell Segmentation</h3>
<p>3D cell segmentations identify and delineate individual cells (in other words, it is instance segmentation) within 3 dimensional microscopy data. Biologists use segmentations to quantitatively analyze cell shapes, cell morphology, and spatial relationships. 3D cell segmentations are crucial for studying embryonic development as they can help reveal the mutual effects of cellular structure (organization) and cell fate differentiation <span class="citation" data-cites="chalifoux2025geometric">(<a href="#ref-chalifoux2025geometric" role="doc-biblioref">Chalifoux, Avdeeva, and Posfai 2025</a>)</span>.</p>
</section>
<section id="problem-overview" class="level3">
<h3 class="anchored" data-anchor-id="problem-overview">Problem Overview</h3>
<br>
<table>
<tbody><tr>
<td colspan="2" align="center">
2 different volumes from a Train Distribution. Note, for all 3D volumes, we show a x-y, y-z, and x-z slice, obtained by slicing the corresponding center.
</td>
</tr>
<tr>
<td valign="top">
<img src="traindist_1.png" width="400">
</td>
<td valign="top">
<img src="traindist_2.png" width="400">
</td>
</tr>
</tbody></table>
<p><br></p>
<p>We illustrate the domain gap issue here. Shown above is an example of 2 volumes from a train distribution.</p>
<p><br></p>
<table>
<tbody><tr>
<td align="center">
Predicted test annotations where Train Distribution = Test Distribution
</td>
<td align="center">
Predicted test annotations where Train Distribution ≠ Test Distribution
</td>
</tr>
<tr>
<td valign="top">
<img src="traintestsame.png" width="400">
</td>
<td valign="top">
<img src="traintestdiff.png" width="400">
</td>
</tr>
</tbody></table>
<p><br></p>
<p>We train a 3D segmentation model on the above train distribution and evaluate it on 2 test distributions: 1 test distribution that is equivalent to the train distribution and 1 test distribution that is different to the train distribution. We observe that if the test and train distributions are equal, we obtain good predicted segmentations. However, if the test and train distributions are not equal, the predicted segmentations are poor.</p>
</section>
<section id="embedseg" class="level3">
<h3 class="anchored" data-anchor-id="embedseg">EmbedSeg</h3>
<p>We use the EmbedSeg model for all experiments. We choose EmbedSeg because it processes 3D data directly, unlike other models that consider 2D slices of a 3D volume <span class="citation" data-cites="lalit2022embedseg">(<a href="#ref-lalit2022embedseg" role="doc-biblioref">Lalit, Tomancak, and Jug 2022</a>)</span>.</p>
<figure align="center" class="figure">
<img src="embedseg.png" width="300" class="figure-img">
<figcaption>
Encoder portion of network used in EmbedSeg <span class="citation" data-cites="lalit2022embedseg">(<a href="#ref-lalit2022embedseg" role="doc-biblioref">Lalit, Tomancak, and Jug 2022</a>)</span>
</figcaption>
</figure>
<p>In EmbedSeg, each voxel predicts a spatial embedding (a unique voxel location that represents an object this voxel belongs to). Each segmentation mask consists of all voxels whose embeddings belong to the same cluster of embeddings. The model predicts clustering bandwidths in x, y, z dimensions and a seediness score (which indicates the likelihood a voxel represents an object instance) to help cluster voxels.</p>
</section>
<section id="datasets" class="level3">
<h3 class="anchored" data-anchor-id="datasets">Datasets</h3>
<p>We use the following 3D cell microscopy datasets.</p>
<p>Source (Training) data: {Mouse embryo A, B from the Hiiragi group}</p>
<ul>
<li>88 volumes across developmental stage</li>
<li>voxel size: (1.295, 0.416, 0.416) μm</li>
</ul>
<p>Target (Testing) data: {Mouse-Organoid-Cells-CBG}</p>
<ul>
<li>84 volumes across developmental stage</li>
<li>voxel size: (1.0, 0.1733, 0.1733) μm</li>
</ul>
</section>
<section id="transfer-learning" class="level3">
<h3 class="anchored" data-anchor-id="transfer-learning">Transfer Learning</h3>
<p>The first domain adaptation method we consider is transfer learning.</p>
<p>Transfer learning is a machine learning technique in which the knowledge a model gains from 1 dataset (or task) is used to improve performance on a related dataset (or task). Transfer learning is particularly useful when the target data does not contain a sufficient number of labelled samples <span class="citation" data-cites="zhuang2020comprehensive">(<a href="#ref-zhuang2020comprehensive" role="doc-biblioref">Zhuang et al. 2020</a>)</span>.</p>
<p>To create our baseline model, we train an EmbedSeg model on the source data for 400 epochs. To create our fine-tuned model, we first train an EmbedSeg model on the source data (mouse embryo A) for 200 epochs. We then fine tune the model on the target data for another 200 epochs. We decay the learning rate by linearly changing a small multiplicative factor throughout the 400 epochs. All layers are allowed to train during fine tuning.</p>
<table>
<tbody><tr>
<td align="center" style="font-size: 20px; font-weight: bold;">
Baseline
</td>
<td align="center" style="font-size: 20px; font-weight: bold;">
Fine-tuned with transfer learning
</td>
</tr>
<tr>
<td colspan="2" align="center">
Test IOU (Intersection over Union)
</td>
</tr>
<tr>
<td align="center">
0.656467
</td>
<td align="center">
0.878288
</td>
</tr>
<tr>
<td colspan="2" align="center">
Predicted Test annotations
</td>
</tr>
<tr>
<td valign="top">
<img src="baseline1.png" width="400">
</td>
<td valign="top">
<img src="finetuned.png" width="400">
</td>
</tr>
</tbody></table>
<p><br></p>
<p>As shown by results above, a model fine-tuned with transfer learning greatly outperforms a baseline model just trained on source data. However, a drawback of transfer learning is that it requires ground truth segmentations for the target dataset, which is expensive to obtain even when not many samples are needed. The next domain adaptation method we consider does not require any ground truth segmentations for the target dataset.</p>
</section>
<section id="adversarial-domain-adaptation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="adversarial-domain-adaptation">Adversarial Domain Adaptation</h3>
<p>A common approach to domain adaptation is to learn a neural transformation that maps different domains to a common feature space. Adversarial domain adaptation methods do so by using adversarial learning to minimize the difference between features extracted from source and target domains, allowing a model trained on the source domain to generalize better to the target domain. Specifically, a domain discriminator network tries to distinguish features from the source domain and features from the target domain, and a feature extractor network tries to generate features that confuse the discriminator network. If trained properly, this adversarial learning objective results in domain invariant features <span class="citation" data-cites="ganin2016domain">(<a href="#ref-ganin2016domain" role="doc-biblioref">Ganin et al. 2016</a>)</span>.</p>
<figure align="center" class="figure">
<img src="adda.png" width="800" class="figure-img">
<figcaption>
Overview of Adversarial Discriminative Domain Adaptation <span class="citation" data-cites="tzeng2017adversarial">(<a href="#ref-tzeng2017adversarial" role="doc-biblioref">Tzeng et al. 2017</a>)</span>
</figcaption>
</figure>
<p>Our framework for adversarial domain adaptation is based on Adversarial Discriminative Domain Adaptation <span class="citation" data-cites="tzeng2017adversarial">(<a href="#ref-tzeng2017adversarial" role="doc-biblioref">Tzeng et al. 2017</a>)</span>. As shown in the above figure, a source encoder is first pre-trained on the source domain. Next, a target encoder is trained such that a discriminator network cannot distinguish between features from the source encoder and target encoder. During evaluation, input from the target domain is mapped to the common feature space and the source classifier classifies the mapped features.</p>
<p>For adversarial domain adaptation in the context of 3D cell segmentation, we make some changes. We use 1 encoder for both the source and target domains as most domain adaptation methods seem to use a shared encoder for source and target domains–doing so did not have a meaningful difference in practice.</p>
<p>In the training loop, EmbedSeg’s encoder and decoder are first trained with source data. Next, a discriminator network is trained to distinguish between features extracted from the source domain and features extracted from the target domain. Finally, EmbedSeg’s encoder (the feature extractor) is trained with backward propagation from the discriminator’s error function: the discriminator is fed “fake” labels for target features. This final step is crucial as it motivates the encoder to generate domain invariant features.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Code for adversarial domain adaptation for 3D cell segmentation can be found <a href="https://github.com/donghyunkm/3DCellSegDA/blob/main/cellSegDA/train_adda.py">here</a>.</p>
</div></div><p>During our experiments with adversarial domain adaptation, we found some key insights that may be of use to anyone attempting similar methods. First, the discriminator network needs to be complex enough for the model to successfully distinguish between domains; we used a 4 layer multilayer perceptron. Next, while EmbedSeg was trained with a batch size of 4, we found that domain adaptation (discriminator training and encoder fine-tuning) should be done with a larger batch size (we used 128)–with a small batch size, the discriminator could not identify meaningful differences between domains.</p>
<table>
<tbody><tr>
<td align="center" style="font-size: 20px; font-weight: bold;">
Baseline
</td>
<td align="center" style="font-size: 20px; font-weight: bold;">
Adversarial Domain Adaptation
</td>
</tr>
<tr>
<td colspan="2" align="center">
Test IOU (Intersection over Union)
</td>
</tr>
<tr>
<td align="center">
0.656467
</td>
<td align="center">
0.734249
</td>
</tr>
<tr>
<td colspan="2" align="center">
Predicted Test annotations
</td>
</tr>
<tr>
<td valign="top">
<img src="baseline2.png" width="400">
</td>
<td valign="top">
<img src="domainadapted.png" width="400">
</td>
</tr>
</tbody></table>
<p><br></p>
<p>We see that a domain adapted model outperforms a baseline model just trained on source data. Unfortunately, domain adaptation did not yield good results compared to transfer learning. We hypothesize that this may be because fine-tuning the encoder to generate domain invariant features might cause the extracted features to deviate from those optimal for the original cell segmentation task.</p>
<p>The final domain adaptation method we consider not only does not require any ground truth segmentations for the target dataset, but also has the advantage of increasing the number of useful training samples (useful in that the training samples all follow the same data distribution).</p>
</section>
<section id="style-transfer" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="style-transfer">Style Transfer</h3>
<p>Style transfer fuses the content of one image with the style (e.g., colors, textures, and noise) of another. By transferring the same style to the source domain and target domain, a model trained on the source data can generalize to the target data without needing ground truth for the target data, i.e.&nbsp;zero shot segmentation <span class="citation" data-cites="jing2019neural">(<a href="#ref-jing2019neural" role="doc-biblioref">Jing et al. 2019</a>)</span>. Style transfer could help increase the number of useful training samples for a target dataset (thus leading to better generalization) as multiple different datasets can all be transferred to the same target style.</p>
<p>Inspired by work on zero-shot segmentation in CellStyle <span class="citation" data-cites="yilmaz2025cellstyle">(<a href="#ref-yilmaz2025cellstyle" role="doc-biblioref">Yilmaz et al. 2025</a>)</span>, we adapt a style transfer method based on a large pre-trained latent diffusion model, StyleID <span class="citation" data-cites="chung2024style">(<a href="#ref-chung2024style" role="doc-biblioref">Chung, Hyun, and Heo 2024</a>)</span>. The advantage of this method is that it does not require inference-stage optimization.</p>
<figure align="center" class="figure">
<img src="styleid.png" width="600" class="figure-img">
<figcaption>
Overview of StyleID <span class="citation" data-cites="chung2024style">(<a href="#ref-chung2024style" role="doc-biblioref">Chung, Hyun, and Heo 2024</a>)</span>
</figcaption>
</figure>
<p>As shown above, the method works by substituting the key and value of a content image with those of a style image in self-attention (of the U-Net).</p>
<figure align="center" class="figure">
<img src="latentdiffusion.png" width="600" class="figure-img">
<figcaption>
Architecture of a latent diffusion model <span class="citation" data-cites="rombach2022high">(<a href="#ref-rombach2022high" role="doc-biblioref">Rombach et al. 2022</a>)</span>
</figcaption>
</figure>
<p>The architecture of a latent diffusion model, the model used in StyleID, is shown above. Self-attention layers added to the U-Net allow each region of an image to weigh and combine information from all other regions.</p>
<figure align="center" class="figure">
<img src="2dto3d.png" width="600" class="figure-img">
<figcaption>
Style transfer for 3D volumes
</figcaption>
</figure>
<p>Because there are no publicly available 3D large-scale pre-trained diffusion models, we adapt this 2D method to 3D. As shown in the figure above, we transfer each slice of the 3D cell microscopy volume independently before concatenating them to obtain the style-transferred 3D volume. Note, we transfer both source and target data to a common style. We found that doing so works better in practice (compared to just transferring either the source or target).</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>3D style transfer code can be found <a href="https://github.com/donghyunkm/3DCellSegDA/blob/main/cellSegDA/3d_style_transfer.py">here</a>.</p>
</div></div><p>We provide visualizations of the style transfer process below.</p>
<table>
<tbody><tr>
<td colspan="3" align="center">
Style to transfer (Mouse embryo A from Hiiragi group). 3 volumes are shown to illustrate the different stages of cell development.
</td>
</tr>
<tr>
<td valign="top">
<img src="styletotransfer1.png" width="266">
</td>
<td valign="top">
<img src="styletotransfer2.png" width="266">
</td>
<td valign="top">
<img src="styletotransfer3.png" width="266">
</td>
</tr>
</tbody></table>
<p><br></p>
<p>We choose to transfer the style of the above distribution. Note, we choose 1 x-y slice from the entire set of 3D volumes as the style to transfer.</p>
<table>
<tbody><tr>
<td align="center">
Original target data (Top)
</td>
<td align="center">
Original source data (Top)
</td>
</tr>
<tr>
<td align="center">
Style transferred target data (Bottom)
</td>
<td align="center">
Style transferred source data (Bottom)
</td>
</tr>
<tr>
<td valign="top">
<img src="MO.png" width="400">
</td>
<td valign="top">
<img src="Hiiragi.png" width="400">
</td>
</tr>
</tbody></table>
<p><br></p>
<p>Above are 3D microscopy volumes before and after they were style transferred.</p>
<table class="caption-top table">
<caption>Test IOU (Intersection over Union) of models trained with style transfer</caption>
<colgroup>
<col style="width: 26%">
<col style="width: 9%">
<col style="width: 7%">
<col style="width: 8%">
<col style="width: 23%">
<col style="width: 24%">
</colgroup>
<thead>
<tr class="header">
<th>Approach \ Dataset</th>
<th>Small target dataset</th>
<th>1 source dataset</th>
<th>2 source datasets</th>
<th>1 source dataset + fine tuned on small target dataset</th>
<th>2 source datasets + fine tuned on small target dataset</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Non-Style Transfer</td>
<td>0.62983</td>
<td>0.65131</td>
<td>0.69955</td>
<td>0.77715</td>
<td>0.80361</td>
</tr>
<tr class="even">
<td>Style Transfer</td>
<td>0.57766</td>
<td><strong>0.69545</strong></td>
<td><strong>0.72947</strong></td>
<td>0.78805</td>
<td>0.79788</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>From the above results, we see that when a model isn’t trained on the target data, training a model on style transferred data leads to improved performance.<br>
<br></p>
<table>
<tbody><tr>
<td align="center">
Predicted segmentations from model trained using 1 non-style transferred small target dataset
</td>
<td align="center">
Predicted segmentations from model trained using 2 style transferred source datasets
</td>
</tr>
<tr>
<td valign="top">
<img src="st_baseline.png" width="400">
</td>
<td valign="top">
<img src="st_good.png" width="400">
</td>
</tr>
</tbody></table>
<p><br> Additionally, we qualitatively confirm these results here. We find that 3D style transfer can improve cell segmentation accuracy, especially when ground truth annotations for target data are scarce or unavailable. However, style transfer only improves segmentation when cells are visually similar. We also find that style transfer introduces noise for slices that do not contain any cells. This could be resolved by injecting different style features for different sections of a 3D volume.</p>
</section>
<section id="conclusion-and-future-work" class="level3">
<h3 class="anchored" data-anchor-id="conclusion-and-future-work">Conclusion and Future Work</h3>
<p>We explored 3 different domain adaptation methods for 3D cell segmentation. We find that all 3 methods have benefits for specific situations. Transfer learning will lead to the best performance when ground truth segmentations for the target dataset are available. On the other hand, domain adaptation and style transfer will lead to the best performance when ground truth segmentations are unavailable.</p>
<p>We believe that style transfer may be the optimal choice for 3D cell segmentation as style transfer could be used to increase the number of useful training samples. We hope to pursue future work in this idea. Future work could also include using style transfer to build large, pre-trained 3D segmentation models that can be immediately applied to newly imaged cells.</p>
<p>One limitation of our work is that our predicted segmentations are not good enough to be used by biologists without additional corrections. However, we believe this to be the result of the training data quality.</p>
<p><br>
</p>
<p>The poster can be found <a href="https://github.com/donghyunkm/3DCellSegDA/blob/main/DonghyunKim_Poster_3648.pdf">here</a>.</p>
<p>Code repository: <a href="https://github.com/donghyunkm/3DCellSegDA" class="uri">https://github.com/donghyunkm/3DCellSegDA</a></p>
<p><br>
</p>
<p>We thank the Hiiragi Group for providing mouse embryo data and annotations. We also thank Alf Honigmann and Anna Goncharova for the Mouse-Organoid-Cells-CBG data and annotations.</p>
<p>We thank Hayden Nunley, David Denberg, Lisa Brown, and Andrew Lu for helpful feedback and discussions.</p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-chalifoux2025geometric" class="csl-entry" role="listitem">
Chalifoux, Madeleine, Maria Avdeeva, and Eszter Posfai. 2025. <span>“Geometric, Cell Cycle and Maternal-to-Zygotic Transition-Associated YAP Dynamics During Preimplantation Embryo Development.”</span> <em>Developmental Biology</em>.
</div>
<div id="ref-chung2024style" class="csl-entry" role="listitem">
Chung, Jiwoo, Sangeek Hyun, and Jae-Pil Heo. 2024. <span>“Style Injection in Diffusion: A Training-Free Approach for Adapting Large-Scale Diffusion Models for Style Transfer.”</span> In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 8795–805.
</div>
<div id="ref-ganin2016domain" class="csl-entry" role="listitem">
Ganin, Yaroslav, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario March, and Victor Lempitsky. 2016. <span>“Domain-Adversarial Training of Neural Networks.”</span> <em>Journal of Machine Learning Research</em> 17 (59): 1–35.
</div>
<div id="ref-jing2019neural" class="csl-entry" role="listitem">
Jing, Yongcheng, Yezhou Yang, Zunlei Feng, Jingwen Ye, Yizhou Yu, and Mingli Song. 2019. <span>“Neural Style Transfer: A Review.”</span> <em>IEEE Transactions on Visualization and Computer Graphics</em> 26 (11): 3365–85.
</div>
<div id="ref-lalit2022embedseg" class="csl-entry" role="listitem">
Lalit, Manan, Pavel Tomancak, and Florian Jug. 2022. <span>“Embedseg: Embedding-Based Instance Segmentation for Biomedical Microscopy Data.”</span> <em>Medical Image Analysis</em> 81: 102523.
</div>
<div id="ref-rombach2022high" class="csl-entry" role="listitem">
Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. <span>“High-Resolution Image Synthesis with Latent Diffusion Models.”</span> In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 10684–95.
</div>
<div id="ref-stringer2021cellpose" class="csl-entry" role="listitem">
Stringer, Carsen, Tim Wang, Michalis Michaelos, and Marius Pachitariu. 2021. <span>“Cellpose: A Generalist Algorithm for Cellular Segmentation.”</span> <em>Nature Methods</em> 18 (1): 100–106.
</div>
<div id="ref-tzeng2017adversarial" class="csl-entry" role="listitem">
Tzeng, Eric, Judy Hoffman, Kate Saenko, and Trevor Darrell. 2017. <span>“Adversarial Discriminative Domain Adaptation.”</span> In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em>, 7167–76.
</div>
<div id="ref-yilmaz2025cellstyle" class="csl-entry" role="listitem">
Yilmaz, Rüveyda, Zhu Chen, Yuli Wu, and Johannes Stegmaier. 2025. <span>“CellStyle: Improved Zero-Shot Cell Segmentation via Style Transfer.”</span> <em>arXiv Preprint arXiv:2503.08603</em>.
</div>
<div id="ref-zhuang2020comprehensive" class="csl-entry" role="listitem">
Zhuang, Fuzhen, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. 2020. <span>“A Comprehensive Survey on Transfer Learning.”</span> <em>Proceedings of the IEEE</em> 109 (1): 43–76.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>